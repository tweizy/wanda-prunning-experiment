{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: torchaudio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.4.2)\n",
      "Requirement already satisfied: accelerate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.23.4)\n",
      "Requirement already satisfied: lm-eval in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.4.9.2)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: seaborn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: evaluate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (0.4.6)\n",
      "Requirement already satisfied: jsonlines in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (4.0.0)\n",
      "Requirement already satisfied: numexpr in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (2.14.1)\n",
      "Requirement already satisfied: peft>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (0.18.0)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (3.0.1)\n",
      "Requirement already satisfied: pytablewriter in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (1.2.1)\n",
      "Requirement already satisfied: rouge-score>=0.0.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (0.1.2)\n",
      "Requirement already satisfied: sacrebleu>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (1.3.2)\n",
      "Requirement already satisfied: sqlitedict in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (2.1.0)\n",
      "Requirement already satisfied: tqdm-multiprocess in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (0.0.11)\n",
      "Requirement already satisfied: zstandard in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (0.25.0)\n",
      "Requirement already satisfied: word2number in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (1.1)\n",
      "Requirement already satisfied: more_itertools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from lm-eval) (10.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: absl-py in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm-eval) (2.3.1)\n",
      "Requirement already satisfied: nltk in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm-eval) (3.9.2)\n",
      "Requirement already satisfied: portalocker in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval) (6.0.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk->rouge-score>=0.0.4->lm-eval) (8.3.1)\n",
      "Requirement already satisfied: DataProperty<2,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pytablewriter->lm-eval) (1.1.0)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pytablewriter->lm-eval) (1.1.4)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pytablewriter->lm-eval) (3.3.1)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pytablewriter->lm-eval) (1.3.4)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pytablewriter->lm-eval) (0.1.7)\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval) (1.3.4)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval) (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio transformers datasets accelerate sentencepiece protobuf lm-eval numpy pandas matplotlib seaborn tqdm huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in to HuggingFace\n",
      "✓ lm-evaluation-harness imported successfully\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# WANDA PRUNING EXPERIMENT\n",
    "# Paper: \"A Simple and Effective Pruning Approach for Large Language Models\"\n",
    "# Models: LLaMA-2-7B, LLaMA-3-8B\n",
    "# Assignment: Demonstrate Wanda > Magnitude Pruning\n",
    "# By: Zineb Abercha & Omar Alfarouq Bouhadi\n",
    "# ==========================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from huggingface_hub import login\n",
    "HF_TOKEN = \"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\"  \n",
    "login(token=HF_TOKEN)\n",
    "print(\"✓ Logged in to HuggingFace\")\n",
    "\n",
    "# Import LM Evaluation Harness\n",
    "try:\n",
    "    from lm_eval import evaluator\n",
    "    from lm_eval.models.huggingface import HFLM\n",
    "    print(\"✓ lm-evaluation-harness imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: lm-evaluation-harness not installed! Run: pip install lm-eval\")\n",
    "\n",
    "# Constants\n",
    "DATASET_ID = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "NSAMPLES = 64          \n",
    "SEQ_LEN = 4096 \n",
    "\n",
    "# Experiment Configuration (aligned with Wanda paper)\n",
    "SPARSITY_RATIOS = [0.3, 0.5, 0.7] \n",
    "ZERO_SHOT_TASKS = [\"piqa\", \"hellaswag\", \"arc_easy\", \"boolq\", \"rte\"]\n",
    "SEED = 0\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- UTILITIES ---\n",
    "\n",
    "def get_wikitext2(tokenizer, nsamples, seq_len):\n",
    "    print(f\"\\n[Data] Loading {DATASET_ID}...\")\n",
    "    traindata = load_dataset(DATASET_ID, DATASET_CONFIG, split='train')\n",
    "    testdata = load_dataset(DATASET_ID, DATASET_CONFIG, split='test')\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "    import random\n",
    "    random.seed(SEED)\n",
    "    trainloader = []\n",
    "    print(f\"[Data] Selecting {nsamples} calibration sequences...\")\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seq_len - 1)\n",
    "        j = i + seq_len\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader, testenc\n",
    "\n",
    "def find_layers(module, layers=[nn.Linear], name=''):\n",
    "    \"\"\"Recursively find linear layers (aligned with official Wanda repo).\"\"\"\n",
    "    if type(module) in layers: \n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))\n",
    "    return res\n",
    "\n",
    "def check_sparsity(model):\n",
    "    \"\"\"Verify actual sparsity matches target (from official Wanda repo).\"\"\"\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "    count = 0 \n",
    "    total_params = 0\n",
    "    layer_sparsities = []\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "        sub_count = 0\n",
    "        sub_params = 0\n",
    "        for name in subset:\n",
    "            W = subset[name].weight.data\n",
    "            count += (W==0).sum().item()\n",
    "            total_params += W.numel()\n",
    "            sub_count += (W==0).sum().item()\n",
    "            sub_params += W.numel()\n",
    "        layer_sparsities.append(float(sub_count)/sub_params if sub_params > 0 else 0)\n",
    "    \n",
    "    model.config.use_cache = use_cache\n",
    "    return float(count)/total_params, layer_sparsities\n",
    "\n",
    "# --- PRUNER CLASS ---\n",
    "\n",
    "class WandaPruner:\n",
    "    def __init__(self, model_id):\n",
    "        self.model_id = model_id\n",
    "        self.model_name = model_id.split('/')[-1]\n",
    "        self.is_llama3 = \"Llama-3\" in model_id or \"llama-3\" in model_id\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.model = None\n",
    "        self.original_state_dict = None\n",
    "        self.pruning_history = []\n",
    "        self.timing_stats = defaultdict(float)\n",
    "    \n",
    "    def load_model(self):\n",
    "        print(f\"\\n[Model] Loading {self.model_id}...\")\n",
    "        start = time.time()\n",
    "        model_kwargs = {\"torch_dtype\": torch.float16, \"device_map\": \"cuda\", \"low_cpu_mem_usage\": True}\n",
    "        if self.is_llama3: \n",
    "            model_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_id, **model_kwargs)\n",
    "        self.model.seqlen = SEQ_LEN\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"[Model] Total Parameters: {total_params:,}\")\n",
    "        \n",
    "        # CPU Caching for Fast Resets\n",
    "        print(\"[Model] Caching weights to CPU...\")\n",
    "        self.original_state_dict = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "        \n",
    "        load_time = time.time() - start\n",
    "        self.timing_stats['model_load'] = load_time\n",
    "        print(f\"[Model] Loaded in {load_time:.2f}s\")\n",
    "        return total_params\n",
    "    \n",
    "    def reset_model(self):\n",
    "        print(\"\\n[Model] Resetting model weights from cache...\")\n",
    "        self.model.load_state_dict(self.original_state_dict)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def prune(self, method=\"wanda\", sparsity=0.5, dataloader=None):\n",
    "        \"\"\"\n",
    "        Prune model using Wanda or Magnitude method.\n",
    "        Implementation verified against official Wanda repo (locuslab/wanda).\n",
    "        \"\"\"\n",
    "        if not self.model: self.load_model()\n",
    "        print(f\"\\n{'='*40}\\nPRUNING: {method.upper()} @ {sparsity:.0%}\\n{'='*40}\")\n",
    "        \n",
    "        prune_start = time.time()\n",
    "        use_cache = self.model.config.use_cache\n",
    "        self.model.config.use_cache = False\n",
    "        \n",
    "        if hasattr(self.model, 'model'): \n",
    "            layers = self.model.model.layers\n",
    "        else: \n",
    "            layers = self.model.transformer.h\n",
    "\n",
    "        dtype = next(iter(self.model.parameters())).dtype\n",
    "        inps = torch.zeros((NSAMPLES, SEQ_LEN, self.model.config.hidden_size), dtype=dtype, device=DEVICE)\n",
    "        cache = {'i': 0, 'args': None, 'kwargs': None}\n",
    "        \n",
    "        class Catcher(nn.Module):\n",
    "            def __init__(self, module):\n",
    "                super().__init__(); self.module = module\n",
    "            def forward(self, inp, *args, **kwargs):\n",
    "                inps[cache['i']] = inp; cache['i'] += 1\n",
    "                cache['args'] = args; cache['kwargs'] = kwargs\n",
    "                raise ValueError\n",
    "        \n",
    "        # Capture inputs (Phase 1)\n",
    "        layers[0] = Catcher(layers[0])\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Calibration\")):\n",
    "            try: \n",
    "                self.model(batch[0].to(DEVICE))\n",
    "            except ValueError: \n",
    "                pass\n",
    "        layers[0] = layers[0].module\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        forward_args = cache['args'] if cache['args'] else ()\n",
    "        forward_kwargs = cache['kwargs'] if cache['kwargs'] else {}\n",
    "        \n",
    "        # Layer-by-layer pruning (Phase 2)\n",
    "        print(\"\\n[Phase 2] Layer-by-layer pruning...\")\n",
    "        stats = {\n",
    "            \"total_params\": 0, \n",
    "            \"pruned_params\": 0, \n",
    "            \"layer_sparsities\": [],\n",
    "            \"layer_names\": []\n",
    "        }\n",
    "        \n",
    "        for layer_idx, layer in enumerate(tqdm(layers, desc=\"Processing Layers\")):\n",
    "            subset = find_layers(layer)\n",
    "            subset_sq_sums = {}\n",
    "            \n",
    "            # Compute activation norms for Wanda (matches official repo)\n",
    "            if method == \"wanda\":\n",
    "                def hook(name): \n",
    "                    def fn(m, i, o):\n",
    "                        inp = i[0].reshape(-1, i[0].shape[-1])\n",
    "                        sq = inp.pow(2).sum(dim=0)  # Sum of squares per feature\n",
    "                        subset_sq_sums[name] = subset_sq_sums.get(name, 0) + sq\n",
    "                    return fn\n",
    "                handles = [subset[n].register_forward_hook(hook(n)) for n in subset]\n",
    "                for j in range(NSAMPLES):\n",
    "                    with torch.no_grad(): \n",
    "                        layer(inps[j].unsqueeze(0), *forward_args, **forward_kwargs)\n",
    "                for h in handles: \n",
    "                    h.remove()\n",
    "            \n",
    "            # Prune each linear layer\n",
    "            layer_zeros = 0\n",
    "            layer_total = 0\n",
    "            for name in subset:\n",
    "                W = subset[name].weight.data\n",
    "                if method == \"wanda\":\n",
    "                    # Wanda metric: |W| * sqrt(sum of squared activations)\n",
    "                    # Verified equivalent to official: |W| * sqrt(scaler_row)\n",
    "                    norms = torch.sqrt(subset_sq_sums[name] + 1e-6)\n",
    "                    metric = torch.abs(W) * norms.reshape(1, -1)\n",
    "                else: \n",
    "                    # Magnitude pruning baseline\n",
    "                    metric = torch.abs(W)\n",
    "                \n",
    "                # Compute threshold (stable sort as per official repo)\n",
    "                thresh = torch.sort(metric, dim=1, stable=True)[0]\n",
    "                thresh = thresh[:, int(W.shape[1] * sparsity)]\n",
    "                mask = metric > thresh.unsqueeze(1)\n",
    "                W.mul_(mask)\n",
    "                \n",
    "                zeros = (W == 0).sum().item()\n",
    "                total = W.numel()\n",
    "                stats[\"total_params\"] += total\n",
    "                stats[\"pruned_params\"] += zeros\n",
    "                layer_zeros += zeros\n",
    "                layer_total += total\n",
    "            \n",
    "            stats[\"layer_sparsities\"].append(layer_zeros / layer_total if layer_total > 0 else 0)\n",
    "            stats[\"layer_names\"].append(f\"Layer {layer_idx}\")\n",
    "            \n",
    "            # Forward pass through pruned layer\n",
    "            for j in range(NSAMPLES):\n",
    "                with torch.no_grad(): \n",
    "                    inps[j] = layer(inps[j].unsqueeze(0), *forward_args, **forward_kwargs)[0]\n",
    "        \n",
    "        self.model.config.use_cache = use_cache\n",
    "        stats[\"actual_sparsity\"] = stats[\"pruned_params\"] / stats[\"total_params\"]\n",
    "        \n",
    "        prune_time = time.time() - prune_start\n",
    "        self.timing_stats[f'prune_{method}_{int(sparsity*100)}'] = prune_time\n",
    "        \n",
    "        # Verify sparsity matches target\n",
    "        verified_sparsity, _ = check_sparsity(self.model)\n",
    "        print(f\"[Verification] Target: {sparsity:.2%}, Actual: {verified_sparsity:.2%}\")\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# --- EVALUATION ---\n",
    "\n",
    "def eval_perplexity(model, testenc, stride=SEQ_LEN):\n",
    "    \"\"\"Evaluate perplexity on WikiText-2 (aligned with official Wanda repo).\"\"\"\n",
    "    print(\"\\n[Perplexity] Evaluating on WikiText-2...\")\n",
    "    eval_start = time.time()\n",
    "    model.eval()\n",
    "    input_ids = testenc.input_ids.to(DEVICE)\n",
    "    seq_len = input_ids.size(1)\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    \n",
    "    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Perplexity\"):\n",
    "        end_loc = min(begin_loc + SEQ_LEN, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc\n",
    "        inp = input_ids[:, begin_loc:end_loc]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-trg_len] = -100\n",
    "        if inp.size(1) == 0: break\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inp, labels=tar)\n",
    "            nlls.append(outputs.loss * trg_len)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len: break\n",
    "    \n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc).item()\n",
    "    eval_time = time.time() - eval_start\n",
    "    print(f\"Perplexity: {ppl:.4f} (eval time: {eval_time:.1f}s)\")\n",
    "    return ppl, eval_time\n",
    "\n",
    "def eval_zero_shot_with_harness(model, tokenizer, model_name, tasks=ZERO_SHOT_TASKS, limit=None):\n",
    "    \"\"\"Evaluate zero-shot accuracy using lm-evaluation-harness.\"\"\"\n",
    "    print(f\"\\n[Zero-Shot] Tasks: {tasks}\")\n",
    "    eval_start = time.time()\n",
    "    \n",
    "    # L40S optimized batch size\n",
    "    lm = HFLM(pretrained=model, tokenizer=tokenizer, batch_size=\"auto\", max_batch_size=64, device=\"cuda\")\n",
    "    results = evaluator.simple_evaluate(model=lm, tasks=tasks, num_fewshot=0, limit=limit, log_samples=False)\n",
    "    \n",
    "    task_results = {}\n",
    "    for task in tasks:\n",
    "        if task in results['results']:\n",
    "            data = results['results'][task]\n",
    "            val = 0.0\n",
    "            # Try multiple key formats from lm-eval\n",
    "            for k, v in data.items():\n",
    "                if k in ['acc_norm,none', 'acc,none', 'acc_norm', 'acc', 'accuracy']: \n",
    "                    val = v\n",
    "                    break\n",
    "            if val == 0.0:\n",
    "                for k, v in data.items():\n",
    "                    if not k.endswith('stderr') and not k =='alias':\n",
    "                        try: \n",
    "                            float(v)\n",
    "                            val = v\n",
    "                            break\n",
    "                        except: \n",
    "                            continue\n",
    "            task_results[task] = float(val)\n",
    "            print(f\"  {task}: {task_results[task]*100:.2f}%\")\n",
    "    \n",
    "    avg = np.mean(list(task_results.values()))\n",
    "    task_results['average'] = avg\n",
    "    \n",
    "    eval_time = time.time() - eval_start\n",
    "    print(f\"Average: {avg*100:.2f}% (eval time: {eval_time:.1f}s)\")\n",
    "    return task_results, eval_time\n",
    "\n",
    "# --- ENHANCED PLOTTING (8 PLOTS) ---\n",
    "\n",
    "def plot_results(results, output_dir):\n",
    "    \"\"\"Generate comprehensive visualization suite (8 plots total).\"\"\"\n",
    "    print(f\"\\n{'='*80}\\nGENERATING VISUALIZATIONS -> {output_dir}\\n{'='*80}\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    n_models = len(results)\n",
    "    model_names = list(results.keys())\n",
    "    \n",
    "    # PLOT 1: Perplexity Comparison (Bar Chart)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 6))\n",
    "    if n_models == 1: axes = [axes]\n",
    "    for idx, (model_name, ax) in enumerate(zip(model_names, axes)):\n",
    "        ppl_data = results[model_name]['perplexity']\n",
    "        methods = ['Dense']\n",
    "        values = [ppl_data.get('dense', 0)]\n",
    "        colors = ['#2ecc71']\n",
    "        for s in SPARSITY_RATIOS:\n",
    "            s_int = int(s*100)\n",
    "            methods.extend([f'Mag\\n{s_int}%', f'Wanda\\n{s_int}%'])\n",
    "            values.extend([ppl_data.get(f'magnitude_{s_int}', 0), ppl_data.get(f'wanda_{s_int}', 0)])\n",
    "            colors.extend(['#e74c3c', '#3498db'])\n",
    "        bars = ax.bar(methods, values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        ax.set_ylabel('Perplexity (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_name}\\nPerplexity vs Sparsity', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2f}', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, '1_perplexity_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # PLOT 2: Performance Degradation (Line Plot)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 6))\n",
    "    if n_models == 1: axes = [axes]\n",
    "    for idx, (model_name, ax) in enumerate(zip(model_names, axes)):\n",
    "        ppl_data = results[model_name]['perplexity']\n",
    "        base_ppl = ppl_data.get('dense', 1)\n",
    "        sparsities = [int(s*100) for s in SPARSITY_RATIOS]\n",
    "        mag_deg = [(ppl_data.get(f'magnitude_{s}', base_ppl) - base_ppl)/base_ppl*100 for s in sparsities]\n",
    "        wanda_deg = [(ppl_data.get(f'wanda_{s}', base_ppl) - base_ppl)/base_ppl*100 for s in sparsities]\n",
    "        ax.plot(sparsities, mag_deg, marker='s', label='Magnitude', color='#e74c3c', linewidth=3, markersize=10)\n",
    "        ax.plot(sparsities, wanda_deg, marker='o', label='Wanda', color='#3498db', linewidth=3, markersize=10)\n",
    "        ax.set_ylabel('Perplexity Increase (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Sparsity (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_name}\\nPerformance Degradation', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=11, loc='best')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        # Add improvement annotation\n",
    "        if len(sparsities) > 0:\n",
    "            for i, s in enumerate(sparsities):\n",
    "                improvement = mag_deg[i] - wanda_deg[i]\n",
    "                if improvement > 0:\n",
    "                    ax.annotate(f'+{improvement:.1f}pp', \n",
    "                               xy=(s, wanda_deg[i]), \n",
    "                               xytext=(s, wanda_deg[i] - 2),\n",
    "                               ha='center', fontsize=9, color='green', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, '2_degradation.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # PLOT 3: Zero-Shot Summary (Bar Chart)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 6))\n",
    "    if n_models == 1: axes = [axes]\n",
    "    for idx, (model_name, ax) in enumerate(zip(model_names, axes)):\n",
    "        zs_data = results[model_name]['zero_shot']\n",
    "        methods = ['Dense']\n",
    "        values = [zs_data.get('dense', {}).get('average', 0)*100]\n",
    "        colors = ['#2ecc71']\n",
    "        for s in SPARSITY_RATIOS:\n",
    "            s_int = int(s*100)\n",
    "            methods.extend([f'Mag {s_int}%', f'Wanda {s_int}%'])\n",
    "            values.extend([\n",
    "                zs_data.get(f'magnitude_{s_int}', {}).get('average', 0)*100, \n",
    "                zs_data.get(f'wanda_{s_int}', {}).get('average', 0)*100\n",
    "            ])\n",
    "            colors.extend(['#e74c3c', '#3498db'])\n",
    "        bars = ax.bar(methods, values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_name}\\nZero-Shot Accuracy (Average)', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylim([0, 100])\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}%', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, '3_zero_shot_summary.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # PLOT 4: Per-Task Breakdown (All Sparsities)\n",
    "    for s in SPARSITY_RATIOS:\n",
    "        s_int = int(s*100)\n",
    "        fig, axes = plt.subplots(1, n_models, figsize=(8*n_models, 6))\n",
    "        if n_models == 1: axes = [axes]\n",
    "        for idx, (model_name, ax) in enumerate(zip(model_names, axes)):\n",
    "            zs_data = results[model_name]['zero_shot']\n",
    "            tasks = ZERO_SHOT_TASKS\n",
    "            dense_scores = [zs_data.get('dense', {}).get(t, 0)*100 for t in tasks]\n",
    "            mag_scores = [zs_data.get(f'magnitude_{s_int}', {}).get(t, 0)*100 for t in tasks]\n",
    "            wanda_scores = [zs_data.get(f'wanda_{s_int}', {}).get(t, 0)*100 for t in tasks]\n",
    "            \n",
    "            x = np.arange(len(tasks))\n",
    "            width = 0.25\n",
    "            ax.bar(x - width, dense_scores, width, label='Dense', color='#2ecc71', alpha=0.8)\n",
    "            ax.bar(x, mag_scores, width, label=f'Magnitude {s_int}%', color='#e74c3c', alpha=0.8)\n",
    "            ax.bar(x + width, wanda_scores, width, label=f'Wanda {s_int}%', color='#3498db', alpha=0.8)\n",
    "            \n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(tasks, fontsize=11)\n",
    "            ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(f'{model_name}: Task Breakdown ({s_int}% Sparsity)', fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.set_ylim([0, 100])\n",
    "            ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'4_task_breakdown_{s_int}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # PLOT 5: Per-Layer Sparsity Distribution (ENHANCED - Magnitude vs Wanda)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(12*n_models, 6))\n",
    "    if n_models == 1: axes = [axes]\n",
    "    for idx, (model_name, ax) in enumerate(zip(model_names, axes)):\n",
    "        # Compare Magnitude vs Wanda at 50% sparsity\n",
    "        stats_mag = results[model_name]['sparsity_stats'].get('magnitude_50', {})\n",
    "        stats_wanda = results[model_name]['sparsity_stats'].get('wanda_50', {})\n",
    "        \n",
    "        if 'layer_sparsities' in stats_mag and 'layer_sparsities' in stats_wanda:\n",
    "            mag_sparsities = np.array(stats_mag['layer_sparsities']) * 100\n",
    "            wanda_sparsities = np.array(stats_wanda['layer_sparsities']) * 100\n",
    "            layers_idx = range(len(mag_sparsities))\n",
    "            \n",
    "            # Plot both methods\n",
    "            ax.plot(layers_idx, mag_sparsities, marker='s', linewidth=2.5, \n",
    "                   markersize=6, label='Magnitude 50%', color='#e74c3c', alpha=0.8)\n",
    "            ax.plot(layers_idx, wanda_sparsities, marker='o', linewidth=2.5, \n",
    "                   markersize=6, label='Wanda 50%', color='#3498db', alpha=0.8)\n",
    "            \n",
    "            # Add target sparsity line\n",
    "            ax.axhline(y=50, color='black', linestyle='--', linewidth=2, \n",
    "                      label='Target (50%)', alpha=0.5)\n",
    "            \n",
    "            # Highlight layers with biggest difference\n",
    "            diff = np.abs(mag_sparsities - wanda_sparsities)\n",
    "            max_diff_idx = np.argmax(diff)\n",
    "            if diff[max_diff_idx] > 5:  # Only annotate if difference > 5%\n",
    "                ax.annotate(f'Max Δ: {diff[max_diff_idx]:.1f}%',\n",
    "                           xy=(max_diff_idx, wanda_sparsities[max_diff_idx]),\n",
    "                           xytext=(max_diff_idx, wanda_sparsities[max_diff_idx] + 10),\n",
    "                           arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "                           fontsize=10, fontweight='bold', color='green')\n",
    "        \n",
    "        ax.set_xlabel('Layer Index', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Sparsity (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_name}\\nPer-Layer Sparsity: Magnitude vs Wanda', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=10, loc='best')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.set_ylim([0, 100])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, '5_layer_sparsity_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # PLOT 6: Wanda vs Magnitude Heatmap\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 6))\n",
    "    if n_models == 1: axes = [axes]\n",
    "    for idx, (model_name, ax) in enumerate(zip(model_names, axes)):\n",
    "        ppl_data = results[model_name]['perplexity']\n",
    "        comparison_matrix = []\n",
    "        for sparsity in SPARSITY_RATIOS:\n",
    "            s_int = int(sparsity*100)\n",
    "            mag_ppl = ppl_data.get(f'magnitude_{s_int}', 0)\n",
    "            wanda_ppl = ppl_data.get(f'wanda_{s_int}', 0)\n",
    "            improvement = ((mag_ppl - wanda_ppl) / mag_ppl) * 100\n",
    "            comparison_matrix.append([mag_ppl, wanda_ppl, improvement])\n",
    "        \n",
    "        comparison_matrix = np.array(comparison_matrix)\n",
    "        im = ax.imshow(comparison_matrix, cmap='RdYlGn', aspect='auto', vmin=0)\n",
    "        \n",
    "        ax.set_xticks([0, 1, 2])\n",
    "        ax.set_xticklabels(['Mag PPL', 'Wanda PPL', 'Improvement (%)'], fontsize=11)\n",
    "        ax.set_yticks(range(len(SPARSITY_RATIOS)))\n",
    "        ax.set_yticklabels([f'{int(s*100)}%' for s in SPARSITY_RATIOS], fontsize=11)\n",
    "        ax.set_ylabel('Sparsity Level', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_name}\\nWanda vs Magnitude Heatmap', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for i in range(len(SPARSITY_RATIOS)):\n",
    "            for j in range(3):\n",
    "                text = ax.text(j, i, f'{comparison_matrix[i, j]:.2f}',\n",
    "                             ha=\"center\", va=\"center\", color=\"white\" if j < 2 else \"black\", \n",
    "                             fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, label='Value')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, '6_wanda_vs_magnitude_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # PLOT 7: Accuracy Retention (%)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(7*n_models, 6))\n",
    "    if n_models == 1: axes = [axes]\n",
    "    for idx, (model_name, ax) in enumerate(zip(model_names, axes)):\n",
    "        zs_data = results[model_name]['zero_shot']\n",
    "        base_acc = zs_data.get('dense', {}).get('average', 0) * 100\n",
    "        sparsities = [int(s*100) for s in SPARSITY_RATIOS]\n",
    "        \n",
    "        mag_retention = [(zs_data.get(f'magnitude_{s}', {}).get('average', 0)*100 / base_acc)*100 for s in sparsities]\n",
    "        wanda_retention = [(zs_data.get(f'wanda_{s}', {}).get('average', 0)*100 / base_acc)*100 for s in sparsities]\n",
    "        \n",
    "        ax.plot(sparsities, mag_retention, marker='s', label='Magnitude', color='#e74c3c', linewidth=3, markersize=10)\n",
    "        ax.plot(sparsities, wanda_retention, marker='o', label='Wanda', color='#3498db', linewidth=3, markersize=10)\n",
    "        ax.axhline(y=100, color='green', linestyle='--', linewidth=2, label='Dense Baseline', alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Sparsity (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Accuracy Retention (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_name}\\nZero-Shot Accuracy Retention', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.set_ylim([70, 105])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, '7_accuracy_retention.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # PLOT 8: Timing Breakdown\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    timing_data = []\n",
    "    labels = []\n",
    "    for model_name in model_names:\n",
    "        if 'timing' in results[model_name]:\n",
    "            timing = results[model_name]['timing']\n",
    "            for key, value in timing.items():\n",
    "                if 'prune' in key or 'ppl' in key or 'zero_shot' in key:\n",
    "                    timing_data.append(value)\n",
    "                    labels.append(f\"{model_name}_{key}\")\n",
    "    \n",
    "    if timing_data:\n",
    "        colors_timing = plt.cm.viridis(np.linspace(0, 1, len(timing_data)))\n",
    "        bars = ax.barh(range(len(timing_data)), timing_data, color=colors_timing, alpha=0.8)\n",
    "        ax.set_yticks(range(len(timing_data)))\n",
    "        ax.set_yticklabels(labels, fontsize=9)\n",
    "        ax.set_xlabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Timing Breakdown', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "        \n",
    "        for i, (bar, val) in enumerate(zip(bars, timing_data)):\n",
    "            ax.text(val, i, f' {val:.1f}s', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, '8_timing_breakdown.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"✓ All 8 plots saved.\")\n",
    "\n",
    "def save_detailed_results(results, output_dir):\n",
    "    \"\"\"Export comprehensive results to CSV and JSON.\"\"\"\n",
    "    print(f\"\\n[Export] Saving detailed results to {output_dir}...\")\n",
    "    \n",
    "    # CSV 1: Perplexity Results\n",
    "    ppl_rows = []\n",
    "    for model_name, data in results.items():\n",
    "        ppl_data = data['perplexity']\n",
    "        for key, value in ppl_data.items():\n",
    "            ppl_rows.append({\n",
    "                'Model': model_name,\n",
    "                'Method': key,\n",
    "                'Perplexity': value\n",
    "            })\n",
    "    pd.DataFrame(ppl_rows).to_csv(os.path.join(output_dir, 'perplexity_results.csv'), index=False)\n",
    "    \n",
    "    # CSV 2: Zero-Shot Results (Detailed)\n",
    "    zs_rows = []\n",
    "    for model_name, data in results.items():\n",
    "        zs_data = data['zero_shot']\n",
    "        for config, tasks in zs_data.items():\n",
    "            if isinstance(tasks, dict):\n",
    "                for task, acc in tasks.items():\n",
    "                    zs_rows.append({\n",
    "                        'Model': model_name,\n",
    "                        'Configuration': config,\n",
    "                        'Task': task,\n",
    "                        'Accuracy': acc\n",
    "                    })\n",
    "    pd.DataFrame(zs_rows).to_csv(os.path.join(output_dir, 'zero_shot_results.csv'), index=False)\n",
    "    \n",
    "    # CSV 3: Per-Layer Sparsity\n",
    "    layer_rows = []\n",
    "    for model_name, data in results.items():\n",
    "        if 'sparsity_stats' in data:\n",
    "            for config, stats in data['sparsity_stats'].items():\n",
    "                if 'layer_sparsities' in stats:\n",
    "                    for i, sparsity in enumerate(stats['layer_sparsities']):\n",
    "                        layer_rows.append({\n",
    "                            'Model': model_name,\n",
    "                            'Configuration': config,\n",
    "                            'Layer': i,\n",
    "                            'Sparsity': sparsity\n",
    "                        })\n",
    "    pd.DataFrame(layer_rows).to_csv(os.path.join(output_dir, 'layer_sparsity.csv'), index=False)\n",
    "    \n",
    "    # JSON: Complete Results\n",
    "    with open(os.path.join(output_dir, 'results_complete.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(\"✓ Exported: perplexity_results.csv, zero_shot_results.csv, layer_sparsity.csv, results_complete.json\")\n",
    "\n",
    "def save_final_report(results, output_dir):\n",
    "    \"\"\"Save comprehensive final report to markdown file.\"\"\"\n",
    "    for model_name, data in results.items():\n",
    "        report_path = os.path.join(output_dir, 'final_report.md')\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(f\"# Final Report: {model_name}\\n\\n\")\n",
    "            \n",
    "            ppl = data['perplexity']\n",
    "            f.write(\"## Perplexity Results (WikiText-2)\\n\\n\")\n",
    "            f.write(\"| Method | PPL | Δ from Dense |\\n\")\n",
    "            f.write(\"|--------|-----|--------------|\\n\")\n",
    "            \n",
    "            base = ppl.get('dense', 0)\n",
    "            f.write(f\"| Dense (Baseline) | {base:.4f} | - |\\n\")\n",
    "            \n",
    "            for s in SPARSITY_RATIOS:\n",
    "                s_int = int(s*100)\n",
    "                for m in ['magnitude', 'wanda']:\n",
    "                    key = f\"{m}_{s_int}\"\n",
    "                    val = ppl.get(key, 0)\n",
    "                    deg = ((val - base)/base)*100\n",
    "                    f.write(f\"| {m.title()} {s_int}% | {val:.4f} | {deg:+.2f}% |\\n\")\n",
    "            \n",
    "            # Zero-Shot Summary\n",
    "            zs = data['zero_shot']\n",
    "            f.write(f\"\\n## Zero-Shot Accuracy\\n\\n\")\n",
    "            f.write(f\"*Average across {len(ZERO_SHOT_TASKS)} tasks: {', '.join(ZERO_SHOT_TASKS)}*\\n\\n\")\n",
    "            f.write(\"| Method | Accuracy | Retention |\\n\")\n",
    "            f.write(\"|--------|----------|-----------|\\n\")\n",
    "            \n",
    "            base_acc = zs.get('dense', {}).get('average', 0) * 100\n",
    "            f.write(f\"| Dense (Baseline) | {base_acc:.2f}% | 100.00% |\\n\")\n",
    "            \n",
    "            for s in SPARSITY_RATIOS:\n",
    "                s_int = int(s*100)\n",
    "                for m in ['magnitude', 'wanda']:\n",
    "                    key = f\"{m}_{s_int}\"\n",
    "                    acc = zs.get(key, {}).get('average', 0) * 100\n",
    "                    retention = (acc / base_acc) * 100 if base_acc > 0 else 0\n",
    "                    f.write(f\"| {m.title()} {s_int}% | {acc:.2f}% | {retention:.2f}% |\\n\")\n",
    "            \n",
    "            # Key Findings\n",
    "            f.write(\"\\n## Key Findings\\n\\n\")\n",
    "            for s in SPARSITY_RATIOS:\n",
    "                s_int = int(s*100)\n",
    "                mag_ppl = ppl.get(f'magnitude_{s_int}', 0)\n",
    "                wanda_ppl = ppl.get(f'wanda_{s_int}', 0)\n",
    "                ppl_improvement = ((mag_ppl - wanda_ppl) / mag_ppl) * 100\n",
    "                \n",
    "                mag_acc = zs.get(f'magnitude_{s_int}', {}).get('average', 0) * 100\n",
    "                wanda_acc = zs.get(f'wanda_{s_int}', {}).get('average', 0) * 100\n",
    "                acc_improvement = wanda_acc - mag_acc\n",
    "                \n",
    "                f.write(f\"\\n### {s_int}% Sparsity\\n\\n\")\n",
    "                f.write(f\"- **Perplexity Improvement:** Wanda improves by {ppl_improvement:.2f}% over Magnitude\\n\")\n",
    "                f.write(f\"- **Accuracy Improvement:** Wanda improves by {acc_improvement:.2f} percentage points\\n\")\n",
    "            \n",
    "            # Per-task breakdown\n",
    "            f.write(\"\\n## Per-Task Zero-Shot Breakdown\\n\\n\")\n",
    "            for s in SPARSITY_RATIOS:\n",
    "                s_int = int(s*100)\n",
    "                f.write(f\"\\n### {s_int}% Sparsity\\n\\n\")\n",
    "                f.write(\"| Task | Dense | Magnitude | Wanda |\\n\")\n",
    "                f.write(\"|------|-------|-----------|-------|\\n\")\n",
    "                \n",
    "                for task in ZERO_SHOT_TASKS:\n",
    "                    if task != 'average':\n",
    "                        dense_acc = zs.get('dense', {}).get(task, 0) * 100\n",
    "                        mag_acc = zs.get(f'magnitude_{s_int}', {}).get(task, 0) * 100\n",
    "                        wanda_acc = zs.get(f'wanda_{s_int}', {}).get(task, 0) * 100\n",
    "                        f.write(f\"| {task} | {dense_acc:.2f}% | {mag_acc:.2f}% | {wanda_acc:.2f}% |\\n\")\n",
    "        \n",
    "        print(f\"✓ Saved: {report_path}\")\n",
    "\n",
    "def save_theoretical_analysis(results, output_dir):\n",
    "    \"\"\"Save theoretical analysis to markdown file.\"\"\"\n",
    "    for model_name, data in results.items():\n",
    "        analysis_path = os.path.join(output_dir, 'theoretical_analysis.md')\n",
    "        \n",
    "        with open(analysis_path, 'w') as f:\n",
    "            f.write(f\"# Theoretical Analysis: {model_name}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## 1. Why Wanda Outperforms Magnitude Pruning\\n\\n\")\n",
    "            f.write(\"Wanda (Weight AND Activation) pruning improves upon magnitude-only pruning by \")\n",
    "            f.write(\"incorporating **input activation statistics** into the pruning decision.\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Core Insight\\n\\n\")\n",
    "            f.write(\"A weight's importance ≠ just its magnitude, but rather:\\n\\n\")\n",
    "            f.write(\"```\\nImportance = |Weight| × Input_Activation_Norm\\n```\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Why This Matters\\n\\n\")\n",
    "            f.write(\"- **Magnitude pruning:** Removes smallest weights → assumes magnitude = importance\\n\")\n",
    "            f.write(\"- **Wanda:** Considers BOTH weight size AND how much it's \\\"used\\\" by real data\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Mathematical Formulation\\n\\n\")\n",
    "            f.write(\"```\\n\")\n",
    "            f.write(\"Magnitude metric: S_mag(w_ij) = |w_ij|\\n\")\n",
    "            f.write(\"Wanda metric:     S_wanda(w_ij) = |w_ij| × ||X_j||₂\\n\")\n",
    "            f.write(\"```\\n\\n\")\n",
    "            f.write(\"where `X_j` is the j-th input feature across calibration samples.\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Practical Example\\n\\n\")\n",
    "            f.write(\"Consider two weights:\\n\\n\")\n",
    "            f.write(\"- **Weight A:** Large magnitude (0.5), rarely activated (low X norm)\\n\")\n",
    "            f.write(\"- **Weight B:** Medium magnitude (0.3), frequently activated (high X norm)\\n\\n\")\n",
    "            f.write(\"**Magnitude pruning** → Keeps A, removes B ❌ (wrong!)\\n\\n\")\n",
    "            f.write(\"**Wanda** → Keeps B, removes A ✅ (correct!)\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Result\\n\\n\")\n",
    "            f.write(\"Wanda preserves weights that are actually important for the model's computations \")\n",
    "            f.write(\"on real data, leading to better performance retention.\\n\\n\")\n",
    "            \n",
    "            # Layer-wise behavior\n",
    "            f.write(\"## 2. Layer-Wise Behavior\\n\\n\")\n",
    "            stats_wanda_50 = data['sparsity_stats'].get('wanda_50', {})\n",
    "            if 'layer_sparsities' in stats_wanda_50:\n",
    "                layer_sparsities = np.array(stats_wanda_50['layer_sparsities']) * 100\n",
    "                mean_sparsity = np.mean(layer_sparsities)\n",
    "                std_sparsity = np.std(layer_sparsities)\n",
    "                min_layer = np.argmin(layer_sparsities)\n",
    "                max_layer = np.argmax(layer_sparsities)\n",
    "                \n",
    "                f.write(\"### Observed Layer Statistics (Wanda 50%)\\n\\n\")\n",
    "                f.write(f\"- **Mean sparsity:** {mean_sparsity:.2f}%\\n\")\n",
    "                f.write(f\"- **Std deviation:** {std_sparsity:.2f}%\\n\")\n",
    "                f.write(f\"- **Min sparsity:** {layer_sparsities[min_layer]:.2f}% (Layer {min_layer})\\n\")\n",
    "                f.write(f\"- **Max sparsity:** {layer_sparsities[max_layer]:.2f}% (Layer {max_layer})\\n\\n\")\n",
    "                \n",
    "                f.write(\"### Interpretation\\n\\n\")\n",
    "                early_late = 'lower' if min_layer < len(layer_sparsities)//2 else 'higher'\n",
    "                f.write(f\"- **Lower layers (early):** Tend to have {early_late} sparsity\\n\")\n",
    "                f.write(\"  - More critical for basic feature extraction\\n\")\n",
    "                late_early = 'higher' if max_layer > len(layer_sparsities)//2 else 'lower'\n",
    "                f.write(f\"- **Higher layers (late):** Tend to have {late_early} sparsity\\n\")\n",
    "                f.write(\"  - More redundancy in high-level representations\\n\\n\")\n",
    "                \n",
    "                variation = 'significant' if std_sparsity > 5 else 'relatively uniform'\n",
    "                f.write(f\"Standard deviation of {std_sparsity:.2f}% indicates **{variation}** variation \")\n",
    "                f.write(\"across layers, showing Wanda adapts pruning to layer importance.\\n\\n\")\n",
    "            \n",
    "            # Failure modes\n",
    "            f.write(\"## 3. Failure Modes & Limitations\\n\\n\")\n",
    "            \n",
    "            zs_data = data['zero_shot']\n",
    "            dense_zs = zs_data.get('dense', {})\n",
    "            wanda_50_zs = zs_data.get('wanda_50', {})\n",
    "            \n",
    "            task_degradations = {}\n",
    "            for task in ZERO_SHOT_TASKS:\n",
    "                if task != 'average':\n",
    "                    dense_acc = dense_zs.get(task, 0) * 100\n",
    "                    wanda_acc = wanda_50_zs.get(task, 0) * 100\n",
    "                    degradation = dense_acc - wanda_acc\n",
    "                    task_degradations[task] = degradation\n",
    "            \n",
    "            if task_degradations:\n",
    "                most_affected_task = max(task_degradations, key=task_degradations.get)\n",
    "                least_affected_task = min(task_degradations, key=task_degradations.get)\n",
    "                \n",
    "                f.write(\"### Failure Mode 1: Task-Specific Degradation\\n\\n\")\n",
    "                f.write(f\"- **Most affected task:** {most_affected_task} ({task_degradations[most_affected_task]:.2f}% accuracy drop)\\n\")\n",
    "                f.write(f\"- **Least affected task:** {least_affected_task} ({task_degradations[least_affected_task]:.2f}% accuracy drop)\\n\\n\")\n",
    "                f.write(f\"**Why:** Tasks requiring more complex reasoning (e.g., {most_affected_task}) are more \")\n",
    "                f.write(\"sensitive to pruning. Wanda preserves perplexity well but may lose some higher-order \")\n",
    "                f.write(\"reasoning capacity.\\n\\n\")\n",
    "                \n",
    "                f.write(\"### Failure Mode 2: Calibration Data Dependency\\n\\n\")\n",
    "                f.write(\"Wanda requires calibration data (WikiText-2) to compute activation norms.\\n\\n\")\n",
    "                f.write(\"**Risk:** If calibration data distribution ≠ target task distribution, Wanda may \")\n",
    "                f.write(\"optimize for wrong activations.\\n\\n\")\n",
    "                f.write(\"**Example:** WikiText (formal text) vs Code generation → activation patterns differ\\n\\n\")\n",
    "                f.write(\"**Mitigation:** Use diverse calibration data or task-specific calibration.\\n\\n\")\n",
    "                \n",
    "                f.write(\"### Failure Mode 3: High Sparsity Regime\\n\\n\")\n",
    "                f.write(\"At 70% sparsity and beyond, BOTH magnitude and Wanda degrade significantly.\\n\\n\")\n",
    "                f.write(\"**Root cause:** Too many weights removed → model capacity fundamentally limited\\n\\n\")\n",
    "                \n",
    "                ppl_data = data['perplexity']\n",
    "                dense_ppl = ppl_data.get('dense', 0)\n",
    "                wanda_70_ppl = ppl_data.get('wanda_70', 0)\n",
    "                wanda_50_ppl = ppl_data.get('wanda_50', 0)\n",
    "                \n",
    "                deg_50 = ((wanda_50_ppl - dense_ppl) / dense_ppl) * 100\n",
    "                deg_70 = ((wanda_70_ppl - dense_ppl) / dense_ppl) * 100\n",
    "                \n",
    "                f.write(\"**Observation from results:**\\n\\n\")\n",
    "                f.write(f\"- 50% sparsity: {deg_50:.2f}% perplexity increase (manageable)\\n\")\n",
    "                severity = 'severe' if deg_70 > 50 else 'significant'\n",
    "                f.write(f\"- 70% sparsity: {deg_70:.2f}% perplexity increase ({severity})\\n\\n\")\n",
    "                f.write(\"**Insight:** Wanda helps, but cannot overcome fundamental capacity limits.\\n\\n\")\n",
    "                \n",
    "                f.write(\"### Failure Mode 4: Unstructured Pruning Overhead\\n\\n\")\n",
    "                f.write(\"Wanda (in this implementation) uses **unstructured** pruning → sparse weights \")\n",
    "                f.write(\"scattered throughout matrices.\\n\\n\")\n",
    "                f.write(\"**Problem:** Modern hardware (GPUs, NPUs) doesn't efficiently accelerate unstructured \")\n",
    "                f.write(\"sparsity without specialized kernels.\\n\\n\")\n",
    "                f.write(\"**Real-world implication:** 50% sparsity may only yield ~10-20% speedup, not 2x.\\n\\n\")\n",
    "                f.write(\"**Solution:** Structured pruning (N:M sparsity) trades some accuracy for guaranteed \")\n",
    "                f.write(\"hardware acceleration.\\n\\n\")\n",
    "                \n",
    "                f.write(\"### Failure Mode 5: No Fine-Tuning Recovery\\n\\n\")\n",
    "                f.write(\"This experiment uses \\\"one-shot\\\" pruning without post-pruning fine-tuning.\\n\\n\")\n",
    "                f.write(\"**Limitation:** Model doesn't adapt to sparsity pattern → performance ceiling.\\n\\n\")\n",
    "                f.write(\"**Potential improvement:** Brief fine-tuning (few epochs) could recover significant \")\n",
    "                f.write(\"accuracy, especially at high sparsity.\\n\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"## 4. Practical Recommendations\\n\\n\")\n",
    "            f.write(\"### For Production Deployment\\n\\n\")\n",
    "            f.write(\"1. Use Wanda with structured (2:4 or 4:8) sparsity for hardware efficiency\\n\")\n",
    "            f.write(\"2. Calibrate on data matching your target distribution\\n\")\n",
    "            f.write(\"3. Consider brief fine-tuning after pruning for recovery\\n\")\n",
    "            f.write(\"4. Start conservatively (30-50% sparsity) before pushing higher\\n\")\n",
    "            f.write(\"5. Monitor task-specific degradation, not just overall metrics\\n\\n\")\n",
    "            \n",
    "            f.write(\"### For Research Extensions\\n\\n\")\n",
    "            f.write(\"1. Test with multiple calibration datasets to study distribution sensitivity\\n\")\n",
    "            f.write(\"2. Compare layer-wise vs global sparsity allocation\\n\")\n",
    "            f.write(\"3. Investigate learned pruning schedules (varying sparsity by layer)\\n\")\n",
    "            f.write(\"4. Combine Wanda with quantization for compound compression\\n\")\n",
    "        \n",
    "        print(f\"✓ Saved: {analysis_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# EXPERIMENT ENGINE\n",
    "# ==========================================\n",
    "\n",
    "def run_experiment(model_id, output_dir):\n",
    "    \"\"\"Run comprehensive experiment for a single model.\"\"\"\n",
    "    print(f\"\\n{'!'*80}\\nSTARTING EXPERIMENT\\nModel: {model_id}\\nOutput: {output_dir}\\n{'!'*80}\")\n",
    "    \n",
    "    # Initialize\n",
    "    pruner = WandaPruner(model_id)\n",
    "    pruner.load_model()\n",
    "    trainloader, testenc = get_wikitext2(pruner.tokenizer, NSAMPLES, SEQ_LEN)\n",
    "    \n",
    "    all_results = {\n",
    "        pruner.model_name: {\n",
    "            'perplexity': {}, \n",
    "            'zero_shot': {},\n",
    "            'sparsity_stats': {},\n",
    "            'timing': {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Baseline Evaluation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATING DENSE BASELINE\")\n",
    "    print(\"=\"*80)\n",
    "    base_ppl, ppl_time = eval_perplexity(pruner.model, testenc)\n",
    "    base_zs, zs_time = eval_zero_shot_with_harness(pruner.model, pruner.tokenizer, pruner.model_name)\n",
    "    all_results[pruner.model_name]['perplexity']['dense'] = base_ppl\n",
    "    all_results[pruner.model_name]['zero_shot']['dense'] = base_zs\n",
    "    all_results[pruner.model_name]['timing']['ppl_dense'] = ppl_time\n",
    "    all_results[pruner.model_name]['timing']['zero_shot_dense'] = zs_time\n",
    "\n",
    "    # Pruning Loop\n",
    "    for sparsity in SPARSITY_RATIOS:\n",
    "        s_int = int(sparsity*100)\n",
    "        \n",
    "        # Magnitude Pruning\n",
    "        print(f\"\\n{'='*80}\\nMAGNITUDE PRUNING @ {s_int}%\\n{'='*80}\")\n",
    "        pruner.reset_model()\n",
    "        stats_mag = pruner.prune(\"magnitude\", sparsity, trainloader)\n",
    "        ppl_mag, ppl_time = eval_perplexity(pruner.model, testenc)\n",
    "        zs_mag, zs_time = eval_zero_shot_with_harness(pruner.model, pruner.tokenizer, \"mag\")\n",
    "        \n",
    "        key = f\"magnitude_{s_int}\"\n",
    "        all_results[pruner.model_name]['perplexity'][key] = ppl_mag\n",
    "        all_results[pruner.model_name]['zero_shot'][key] = zs_mag\n",
    "        all_results[pruner.model_name]['sparsity_stats'][key] = stats_mag\n",
    "        all_results[pruner.model_name]['timing'][f'ppl_{key}'] = ppl_time\n",
    "        all_results[pruner.model_name]['timing'][f'zero_shot_{key}'] = zs_time\n",
    "        \n",
    "        # Wanda Pruning\n",
    "        print(f\"\\n{'='*80}\\nWANDA PRUNING @ {s_int}%\\n{'='*80}\")\n",
    "        pruner.reset_model()\n",
    "        stats_wanda = pruner.prune(\"wanda\", sparsity, trainloader)\n",
    "        ppl_wanda, ppl_time = eval_perplexity(pruner.model, testenc)\n",
    "        zs_wanda, zs_time = eval_zero_shot_with_harness(pruner.model, pruner.tokenizer, \"wanda\")\n",
    "        \n",
    "        key = f\"wanda_{s_int}\"\n",
    "        all_results[pruner.model_name]['perplexity'][key] = ppl_wanda\n",
    "        all_results[pruner.model_name]['zero_shot'][key] = zs_wanda\n",
    "        all_results[pruner.model_name]['sparsity_stats'][key] = stats_wanda\n",
    "        all_results[pruner.model_name]['timing'][f'ppl_{key}'] = ppl_time\n",
    "        all_results[pruner.model_name]['timing'][f'zero_shot_{key}'] = zs_time\n",
    "\n",
    "    # Add pruner timing stats\n",
    "    all_results[pruner.model_name]['timing'].update(pruner.timing_stats)\n",
    "    \n",
    "    # Generate outputs\n",
    "    plot_results(all_results, output_dir)\n",
    "    save_detailed_results(all_results, output_dir)\n",
    "    save_final_report(all_results, output_dir)\n",
    "    save_theoretical_analysis(all_results, output_dir)\n",
    "        \n",
    "    # Cleanup\n",
    "    print(\"\\n[Cleanup] Freeing GPU memory...\")\n",
    "    del pruner.model\n",
    "    del pruner\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ Experiment Complete.\\n\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "EXPERIMENT 1: LLaMA-2-7B\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "STARTING EXPERIMENT\n",
      "Model: meta-llama/Llama-2-7b-hf\n",
      "Output: results_llama2\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ea92b9fe364ee883b1b3b8b9c1e307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8713f9dae6da4331aa9fb85758f96d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001ff6b55fe74de4b605163aab031eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98b27a6a9c342109346fdd678aba403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model] Loading meta-llama/Llama-2-7b-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfbe00113e64249ab7d1c0030cf84f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b682ad5738c4e20956d7333046ff6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0e2b1817e14994b9785b22c985f163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcd13393dfb4106a3b1130786870fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be64b64370244b918f67d3f711a1d27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadee2d5f6a1419997fe021f701596ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e0525e8919445fa798832fba36b160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Total Parameters: 6,738,415,616\n",
      "[Model] Caching weights to CPU...\n",
      "[Model] Loaded in 53.67s\n",
      "\n",
      "[Data] Loading wikitext...\n",
      "[Data] Selecting 64 calibration sequences...\n",
      "\n",
      "================================================================================\n",
      "EVALUATING DENSE BASELINE\n",
      "================================================================================\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3988b74a164b8b905a4457952022a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.1140 (eval time: 33.3s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2571.11it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2627.91it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1468.94it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3353.69it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1435.52it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:32<00:00, 117.94it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 78.13%\n",
      "  hellaswag: 57.10%\n",
      "  arc_easy: 75.46%\n",
      "  boolq: 79.33%\n",
      "  rte: 63.90%\n",
      "Average: 70.78% (eval time: 569.6s)\n",
      "\n",
      "================================================================================\n",
      "MAGNITUDE PRUNING @ 30%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: MAGNITUDE @ 30%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fa80e85b0142439ed2f464ac4766f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0543c1968c46469ce57da38114bf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 30.00%, Actual: 30.08%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81318d17f98f4b3aa009117cbdf7cc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.7350 (eval time: 33.1s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2663.03it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2656.81it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1515.71it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3669.75it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1414.12it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:16<00:00, 121.79it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 78.40%\n",
      "  hellaswag: 58.13%\n",
      "  arc_easy: 75.97%\n",
      "  boolq: 75.20%\n",
      "  rte: 57.76%\n",
      "Average: 69.09% (eval time: 550.2s)\n",
      "\n",
      "================================================================================\n",
      "WANDA PRUNING @ 30%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: WANDA @ 30%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99d3b9c109743d29a6f8fe501499955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29864384276400687b4e9bd396a40a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 30.00%, Actual: 30.56%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666bb5370bd14a5f9b98c94923f7f579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.5244 (eval time: 33.4s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2634.96it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2666.53it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1525.31it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3671.87it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1409.01it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:14<00:00, 122.10it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 78.02%\n",
      "  hellaswag: 56.56%\n",
      "  arc_easy: 75.59%\n",
      "  boolq: 78.56%\n",
      "  rte: 56.68%\n",
      "Average: 69.08% (eval time: 548.8s)\n",
      "\n",
      "================================================================================\n",
      "MAGNITUDE PRUNING @ 50%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: MAGNITUDE @ 50%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5289e148c641359a4792e88f2eac6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1fd5a6a56e49c7a4ac8532b4b12fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 50.00%, Actual: 50.13%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486cabb9178644e2a6968c4d9aba7a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 11.8324 (eval time: 32.5s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2623.43it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2666.54it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1468.62it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3657.17it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1385.19it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:04<00:00, 124.64it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 74.59%\n",
      "  hellaswag: 53.09%\n",
      "  arc_easy: 67.85%\n",
      "  boolq: 63.43%\n",
      "  rte: 52.35%\n",
      "Average: 62.26% (eval time: 539.2s)\n",
      "\n",
      "================================================================================\n",
      "WANDA PRUNING @ 50%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: WANDA @ 50%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd271d39d6f4b9e90ddf80a2b9d4ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7aa96784f645d2bb49546bc16cabb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 50.00%, Actual: 50.04%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5362e49da10f46508832acacb5d1be1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 6.4154 (eval time: 32.5s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2545.29it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2637.57it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1505.88it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3568.79it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1407.82it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:00<00:00, 125.87it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 76.06%\n",
      "  hellaswag: 51.82%\n",
      "  arc_easy: 72.56%\n",
      "  boolq: 76.09%\n",
      "  rte: 54.51%\n",
      "Average: 66.21% (eval time: 534.0s)\n",
      "\n",
      "================================================================================\n",
      "MAGNITUDE PRUNING @ 70%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: MAGNITUDE @ 70%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce72cec8d3d54f29a194c31693700dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f89f86c2534d1baa85eb4311f4b6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 70.00%, Actual: 70.16%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f1be512c4f4ac18f918a0b1063d2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: nan (eval time: 31.1s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2624.55it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2649.47it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1507.70it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3584.44it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1408.71it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [07:38<00:00, 131.86it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 52.77%\n",
      "  hellaswag: 25.87%\n",
      "  arc_easy: 25.63%\n",
      "  boolq: 37.86%\n",
      "  rte: 51.99%\n",
      "Average: 38.82% (eval time: 512.7s)\n",
      "\n",
      "================================================================================\n",
      "WANDA PRUNING @ 70%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: WANDA @ 70%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10238aeaa946495cae1c02058064bcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416de77353f9493d90b613af2b30dd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 70.00%, Actual: 70.03%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc0dbb471e748208a873da5079f2440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 68.8045 (eval time: 31.1s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2648.18it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2666.84it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1465.35it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3619.05it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1353.92it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [07:41<00:00, 131.07it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 54.24%\n",
      "  hellaswag: 27.89%\n",
      "  arc_easy: 29.55%\n",
      "  boolq: 47.03%\n",
      "  rte: 52.71%\n",
      "Average: 42.28% (eval time: 514.8s)\n",
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS -> results_llama2\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "posx and posy should be finite values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All 8 plots saved.\n",
      "\n",
      "[Export] Saving detailed results to results_llama2...\n",
      "✓ Exported: perplexity_results.csv, zero_shot_results.csv, layer_sparsity.csv, results_complete.json\n",
      "✓ Saved: results_llama2/final_report.md\n",
      "✓ Saved: results_llama2/theoretical_analysis.md\n",
      "\n",
      "[Cleanup] Freeing GPU memory...\n",
      "✓ Experiment Complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run LLaMA-2\n",
    "    print(\"\\n\\n\" + \"█\"*80)\n",
    "    print(\"EXPERIMENT 1: LLaMA-2-7B\")\n",
    "    print(\"█\"*80)\n",
    "    results_llama2 = run_experiment(\"meta-llama/Llama-2-7b-hf\", \"results_llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "EXPERIMENT 2: LLaMA-3.1-8B\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "STARTING EXPERIMENT\n",
      "Model: meta-llama/Llama-3.1-8B\n",
      "Output: results_llama3\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "[Model] Loading meta-llama/Llama-3.1-8B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedcf143f59341148961860e5b1370e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Total Parameters: 8,030,261,248\n",
      "[Model] Caching weights to CPU...\n",
      "[Model] Loaded in 24.86s\n",
      "\n",
      "[Data] Loading wikitext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Selecting 64 calibration sequences...\n",
      "\n",
      "================================================================================\n",
      "EVALUATING DENSE BASELINE\n",
      "================================================================================\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315073dc0b1b4f4d824e03d89a851f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.8464 (eval time: 72.9s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2434.83it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2453.92it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1456.40it/s]\n",
      "100%|██████████| 10042/10042 [00:03<00:00, 3200.93it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1350.09it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:59<00:00, 112.03it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 79.27%\n",
      "  hellaswag: 60.67%\n",
      "  arc_easy: 82.20%\n",
      "  boolq: 83.09%\n",
      "  rte: 71.12%\n",
      "Average: 75.27% (eval time: 600.3s)\n",
      "\n",
      "================================================================================\n",
      "MAGNITUDE PRUNING @ 30%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: MAGNITUDE @ 30%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba30d119e6b43e8a2bd5008d6bce0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26234ad9c08846fb8702105a01e5f2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 30.00%, Actual: 30.09%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a066f596bc4e4a91293accd288d80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 8.5034 (eval time: 73.1s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2547.44it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2521.87it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1470.69it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3467.60it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1355.24it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:44<00:00, 115.20it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 76.61%\n",
      "  hellaswag: 56.37%\n",
      "  arc_easy: 78.37%\n",
      "  boolq: 80.18%\n",
      "  rte: 67.15%\n",
      "Average: 71.74% (eval time: 586.0s)\n",
      "\n",
      "================================================================================\n",
      "WANDA PRUNING @ 30%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: WANDA @ 30%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c98bbd45d3a4d77b36b66ded61146e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8661ba287d954293bb731f219c853263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 30.00%, Actual: 30.01%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a7c8cbab6c40b28b21fb43bb542eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 6.3185 (eval time: 72.9s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2442.82it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2580.80it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1422.93it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3540.82it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1304.49it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:48<00:00, 114.39it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 78.35%\n",
      "  hellaswag: 59.55%\n",
      "  arc_easy: 80.51%\n",
      "  boolq: 82.94%\n",
      "  rte: 71.12%\n",
      "Average: 74.49% (eval time: 589.4s)\n",
      "\n",
      "================================================================================\n",
      "MAGNITUDE PRUNING @ 50%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: MAGNITUDE @ 50%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb59144af4d74c4fb444a5fd669a80e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defe47458e304c0abca324ce9a2dfd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 50.00%, Actual: 50.15%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1aa54764c564fe1b118e1d3e790b8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 37.8210 (eval time: 72.3s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2519.29it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2595.93it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1457.83it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3484.96it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1308.41it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:31<00:00, 118.05it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 70.46%\n",
      "  hellaswag: 43.27%\n",
      "  arc_easy: 62.21%\n",
      "  boolq: 52.42%\n",
      "  rte: 53.79%\n",
      "Average: 56.43% (eval time: 574.4s)\n",
      "\n",
      "================================================================================\n",
      "WANDA PRUNING @ 50%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: WANDA @ 50%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0578df7cda1946719cb4903a5be29f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626887bc88744417a5185acd01932848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 50.00%, Actual: 50.03%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bf65819e174109bc8b36e93a78a5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 8.9966 (eval time: 72.2s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2485.84it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2549.81it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1455.37it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3439.94it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1352.21it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:28<00:00, 118.93it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 73.88%\n",
      "  hellaswag: 50.53%\n",
      "  arc_easy: 71.84%\n",
      "  boolq: 79.14%\n",
      "  rte: 54.87%\n",
      "Average: 66.05% (eval time: 569.0s)\n",
      "\n",
      "================================================================================\n",
      "MAGNITUDE PRUNING @ 70%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: MAGNITUDE @ 70%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fd54a3a3934eb7a122d5f6d577c545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8ad9ef05d44ca6be60fb0d0ce30d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 70.00%, Actual: 70.14%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6e82f2c18e4cfa82e9e5817d881471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 263197.6250 (eval time: 71.6s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2490.71it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2566.16it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1455.56it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3543.21it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1359.04it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:10<00:00, 123.16it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 54.73%\n",
      "  hellaswag: 26.44%\n",
      "  arc_easy: 29.71%\n",
      "  boolq: 37.83%\n",
      "  rte: 52.71%\n",
      "Average: 40.28% (eval time: 551.9s)\n",
      "\n",
      "================================================================================\n",
      "WANDA PRUNING @ 70%\n",
      "================================================================================\n",
      "\n",
      "[Model] Resetting model weights from cache...\n",
      "\n",
      "========================================\n",
      "PRUNING: WANDA @ 70%\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c23279f1e0d405dbf62f945c24eddde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calibration:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Layer-by-layer pruning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e199f56c645d4eb6963843a555145989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verification] Target: 70.00%, Actual: 70.03%\n",
      "\n",
      "[Perplexity] Evaluating on WikiText-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770852f120f04565bef44f8f99ebccb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 96.7458 (eval time: 71.3s)\n",
      "\n",
      "[Zero-Shot] Tasks: ['piqa', 'hellaswag', 'arc_easy', 'boolq', 'rte']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of rte from None to 0\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 277/277 [00:00<00:00, 2511.50it/s]\n",
      "100%|██████████| 3270/3270 [00:01<00:00, 2587.37it/s]\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1439.93it/s]\n",
      "100%|██████████| 10042/10042 [00:02<00:00, 3491.92it/s]\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1316.90it/s]\n",
      "Running loglikelihood requests:   0%|          | 0/60439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████| 60439/60439 [08:06<00:00, 124.26it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  piqa: 55.17%\n",
      "  hellaswag: 27.48%\n",
      "  arc_easy: 32.32%\n",
      "  boolq: 44.43%\n",
      "  rte: 52.71%\n",
      "Average: 42.42% (eval time: 547.5s)\n",
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS -> results_llama3\n",
      "================================================================================\n",
      "✓ All 8 plots saved.\n",
      "\n",
      "[Export] Saving detailed results to results_llama3...\n",
      "✓ Exported: perplexity_results.csv, zero_shot_results.csv, layer_sparsity.csv, results_complete.json\n",
      "✓ Saved: results_llama3/final_report.md\n",
      "✓ Saved: results_llama3/theoretical_analysis.md\n",
      "\n",
      "[Cleanup] Freeing GPU memory...\n",
      "✓ Experiment Complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run LLaMA-3.1\n",
    "    print(\"\\n\\n\" + \"█\"*80)\n",
    "    print(\"EXPERIMENT 2: LLaMA-3.1-8B\")\n",
    "    print(\"█\"*80)\n",
    "    results_llama3 = run_experiment(\"meta-llama/Llama-3.1-8B\", \"results_llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "ALL EXPERIMENTS COMPLETED SUCCESSFULLY\n",
      "Results saved to: results_llama2/ and results_llama3/\n",
      "████████████████████████████████████████████████████████████████████████████████\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"█\"*80)\n",
    "print(\"ALL EXPERIMENTS COMPLETED SUCCESSFULLY\")\n",
    "print(\"Results saved to: results_llama2/ and results_llama3/\")\n",
    "print(\"█\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
